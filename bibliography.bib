@book{latex,
  title = {LaTeX : A Documentation Preparation System User's Guide and Reference Manual},
  publisher = {Addison-Wesley Professional},
  year = {1994},
  author = {Leslie Lamport}
}

@misc{secim2023,
	title = {\#{Secim2023}: {First} {Public} {Dataset} for {Studying} {Turkish} {General} {Election}},
	shorttitle = {\#{Secim2023}},
	url = {http://arxiv.org/abs/2211.13121},
	abstract = {In the context of Turkey's upcoming parliamentary and presidential elections ("se{\textbackslash}c\{c\}im" in Turkish), social media is playing an important role in shaping public debate. The increasing engagement of citizens on social media platforms has led to the growing use of social media by political actors. It is of utmost importance to capture the upcoming Turkish elections, as social media is becoming an essential component of election propaganda, political debates, smear campaigns, and election manipulation by domestic and international actors. We provide a comprehensive dataset for social media researchers to study the upcoming election, develop tools to prevent online manipulation, and gather novel information to inform the public. We are committed to continually improving the data collection and updating it regularly leading up to the election. Using the Secim2023 dataset, researchers can examine the social and communication networks between political actors, track current trends, and investigate emerging threats to election integrity. Our dataset is available at: https://github.com/ViralLab/Secim2023\_Dataset},
	urldate = {2023-10-28},
	publisher = {arXiv},
	author = {Najafi, Ali and Mugurtay, Nihat and Demirci, Ege and Demirkiran, Serhat and Karadeniz, Huseyin Alper and Varol, Onur},
	month = nov,
	year = {2022},
}

@misc{turkishbertweet_2023,
	title = {{TurkishBERTweet}: {Fast} and {Reliable} {Large} {Language} {Model} for {Social} {Media} {Analysis}},
	shorttitle = {{TurkishBERTweet}},
	url = {http://arxiv.org/abs/2311.18063},
	doi = {10.48550/arXiv.2311.18063},
	abstract = {Turkish is one of the most popular languages in the world. Wide us of this language on social media platforms such as Twitter, Instagram, or Tiktok and strategic position of the country in the world politics makes it appealing for the social network researchers and industry. To address this need, we introduce TurkishBERTweet, the first large scale pre-trained language model for Turkish social media built using almost 900 million tweets. The model shares the same architecture as base BERT model with smaller input length, making TurkishBERTweet lighter than BERTurk and can have significantly lower inference time. We trained our model using the same approach for RoBERTa model and evaluated on two text classification tasks: Sentiment Classification and Hate Speech Detection. We demonstrate that TurkishBERTweet outperforms the other available alternatives on generalizability and its lower inference time gives significant advantage to process large-scale datasets. We also compared our models with the commercial OpenAI solutions in terms of cost and performance to demonstrate TurkishBERTweet is scalable and cost-effective solution. As part of our research, we released TurkishBERTweet and fine-tuned LoRA adapters for the mentioned tasks under the MIT License to facilitate future research and applications on Turkish social media. Our TurkishBERTweet model is available at: https://github.com/ViralLab/TurkishBERTweet},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Najafi, Ali and Varol, Onur},
	month = nov,
	year = {2023},
}

@misc{bertopic,
	title = {{BERTopic}: {Neural} topic modeling with a class-based {TF}-{IDF} procedure},
	shorttitle = {{BERTopic}},
	url = {http://arxiv.org/abs/2203.05794},
	abstract = {Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Grootendorst, Maarten},
	month = mar,
	year = {2022},
}

@misc{sentence-bert,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
}

@article{hdbscan,
	title = {hdbscan: {Hierarchical} density based clustering},
	volume = {2},
	issn = {2475-9066},
	shorttitle = {hdbscan},
	url = {https://joss.theoj.org/papers/10.21105/joss.00205},
	doi = {10.21105/joss.00205},
	abstract = {McInnes et al, (2017), hdbscan: Hierarchical density based clustering, Journal of Open Source Software, 2(11), 205, doi:10.21105/joss.00205},
	language = {en},
	number = {11},
	urldate = {2024-01-14},
	journal = {Journal of Open Source Software},
	author = {McInnes, Leland and Healy, John and Astels, Steve},
	month = mar,
	year = {2017},
	pages = {205},
}

@article{topic_model_comparison_bertopic_2022,
	title = {A {Topic} {Modeling} {Comparison} {Between} {LDA}, {NMF}, {Top2Vec}, and {BERTopic} to {Demystify} {Twitter} {Posts}},
	volume = {7},
	issn = {2297-7775},
	url = {https://www.frontiersin.org/articles/10.3389/fsoc.2022.886498},
	abstract = {The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms concerning their strengths and weaknesses in a social science context. Based on certain details during the analytical procedures and on quality issues, this research sheds light on the efficacy of using BERTopic and NMF to analyze Twitter data.},
	urldate = {2024-01-13},
	journal = {Frontiers in Sociology},
	author = {Egger, Roman and Yu, Joanne},
	year = {2022},
}

@misc{twitter_topic_modeling_bertopic_lda_2023,
	title = {What {Twitter} {Data} {Tell} {Us} about the {Future}?},
	url = {http://arxiv.org/abs/2308.02035},
	doi = {10.48550/arXiv.2308.02035},
	abstract = {Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modelling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signals futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Landowska, Alina and Robak, Marek and Skorski, Maciej},
	month = jul,
	year = {2023},
}

@mastersthesis{bertopic_twitter_german_politics_2022,
	type = {bat},
	title = {Application of neural topic models to twitter data from {German} politicians},
	url = {https://epub.ub.uni-muenchen.de/92617/},
	language = {eng},
	urldate = {2024-01-29},
	author = {Gritto, Anne},
	month = feb,
	year = {2022},
	doi = {10.5282/ubm/epub.92617},
  school = {Ludwig-Maximilians-Universität München},
}
